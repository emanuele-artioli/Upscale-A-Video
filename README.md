<div align="center">

<h1>
    Upscale-A-Video:<br> 
    Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution
</h1>

<div>
    <a href='https://shangchenzhou.com/' target='_blank'>Shangchen Zhou<sup>âˆ—</sup></a>&emsp;
    <a href='https://pq-yang.github.io/' target='_blank'>Peiqing Yang<sup>âˆ—</sup></a>&emsp;
    <a href='https://iceclear.github.io/' target='_blank'>Jianyi Wang</a>&emsp;
    <a href='https://github.com/Luo-Yihang' target='_blank'>Yihang Luo</a>&emsp;
    <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>
</div>
<div>
    S-Lab, Nanyang Technological University
</div>

<div>
    <strong>CVPR 2024 (Highlight)</strong>
</div>

<div>
    <h4 align="center">
        <a href="https://shangchenzhou.com/projects/upscale-a-video/" target='_blank'>
        <img src="https://img.shields.io/badge/ğŸ³-Project%20Page-blue">
        </a>
        <a href="https://arxiv.org/abs/2312.06640" target='_blank'>
        <img src="https://img.shields.io/badge/arXiv-2312.06640-b31b1b.svg">
        </a>
        <a href="https://www.youtube.com/watch?v=b9J3lqiKnLM" target='_blank'>
        <img src="https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&logoColor=white">
        </a>
        <a href="https://replicate.com/sczhou/upscale-a-video" target='_blank'>
        <img src="https://replicate.com/sczhou/upscale-a-video/badge">
        </a>
        <img src="https://api.infinitescript.com/badgen/count?name=sczhou/Upscale-A-Video">
    </h4>
</div>

<strong>Upscale-A-Video is a diffusion-based model that upscales videos by taking the low-resolution video and text prompts as inputs.</strong>

<div style="width: 100%; text-align: center; margin:auto;">
    <img style="width:100%" src="assets/teaser.png">
</div>

:open_book: For more visual results, go checkout our <a href="##" target="_blank">project page</a>

---
</div>


## ğŸ”¥ Update
- [2024.09] Inference code is released.
- [2024.02] YouHQ dataset is made publicly available.
- [2023.12] This repo is created.

## ğŸ¬ Overview
![overall_structure](assets/pipeline.png)

## ğŸ”§ Dependencies and Installation

### Option 1: Install as a Package (Recommended)

```bash
# Clone the repository
git clone https://github.com/sczhou/Upscale-A-Video.git
cd Upscale-A-Video

# Install as a package in your virtual environment
pip install -e .
```

### Option 2: Traditional Setup

1. Clone Repo
    ```bash
    git clone https://github.com/sczhou/Upscale-A-Video.git
    cd Upscale-A-Video
    ```

2. Create Conda Environment and Install Dependencies
    ```bash
    # create new conda env
    conda create -n UAV python=3.9 -y
    conda activate UAV

    # install python dependencies
    pip install -r requirements.txt
    ```

### Download Models

### Download Models

Download pretrained models and configs from [Google Drive](https://drive.google.com/drive/folders/1O8pbeR1hsRlFUU8O4EULe-lOKNGEWZl1?usp=sharing) and put them under the `pretrained_models/upscale_a_video` folder.

   The [`pretrained_models`](./pretrained_models) directory structure should be arranged as:

    ```
    â”œâ”€â”€ pretrained_models
    â”‚   â”œâ”€â”€ upscale_a_video
    â”‚   â”‚   â”œâ”€â”€ low_res_scheduler
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ propagator
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ scheduler
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ text_encoder
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ tokenizer
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ unet
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚   â”œâ”€â”€ vae
    â”‚   â”‚       â”œâ”€â”€ ...
    ```
    
    (a) (Optional) LLaVA can be downloaded automatically when set `--use_llava` to `True`, for users with access to huggingface.


## ğŸ“¦ Python Package Usage

After installing as a package, you can use Upscale-A-Video directly in your Python code:

```python
from upscale_a_video import UpscaleAVideo
import numpy as np

# Initialize the upscaler
upscaler = UpscaleAVideo(
    pretrained_path="./pretrained_models/upscale_a_video",
    device="cuda:0",
    use_video_vae=False,
    use_propagation=False,
)

# Your input frames: list of numpy arrays (H, W, C), PIL Images, 
# or numpy array (T, H, W, C) in RGB, uint8 [0-255]
frames = [...]  # Your video frames

# Upscale frames (4x resolution)
upscaled_frames = upscaler.upscale_frames(
    frames,
    noise_level=120,        # [0-200], higher = better quality, lower fidelity
    guidance_scale=6.0,     # Higher = more details
    inference_steps=30,     # More steps = higher quality
    output_format="numpy"   # "numpy", "pil", "torch", "numpy_list", "pil_list"
)

# upscaled_frames is now 4x the input resolution
```

### Available Options

```python
upscaled = upscaler.upscale_frames(
    frames,
    prompt="",                      # Optional text prompt
    noise_level=120,                # Noise level [0-200]
    guidance_scale=6.0,             # CFG scale
    inference_steps=30,             # Denoising steps
    propagation_steps=[24, 26, 28], # For temporal consistency (requires use_propagation=True)
    positive_prompt="best quality, extremely detailed",
    negative_prompt="blur, worst quality",
    color_fix="None",               # "None", "AdaIn", or "Wavelet"
    tile_size=256,                  # For large frames
    seed=10,                        # Random seed
    output_format="numpy",          # Output format
)
```

### Context Manager

```python
# Automatically unloads models when done
with UpscaleAVideo("./pretrained_models/upscale_a_video") as upscaler:
    result = upscaler.upscale_frames(frames)
```


## â˜•ï¸ Quick Inference (CLI)

The `--input_path` can be either the path to a single video or a folder containing multiple videos.

We provide several examples in the [`inputs`](./inputs) folder. 
Run the following commands to try it out:

```shell
## AIGC videos
python inference_upscale_a_video.py \
-i ./inputs/aigc_1.mp4 -o ./results -n 150 -g 6 -s 30 -p 24,26,28

python inference_upscale_a_video.py \
-i ./inputs/aigc_2.mp4 -o ./results -n 150 -g 6 -s 30 -p 24,26,28

python inference_upscale_a_video.py \
-i ./inputs/aigc_3.mp4 -o ./results -n 150 -g 6 -s 30 -p 20,22,24
```

```shell
## old videos/movies/animations 
python inference_upscale_a_video.py \
-i ./inputs/old_video_1.mp4 -o ./results -n 150 -g 9 -s 30

python inference_upscale_a_video.py \
-i ./inputs/old_movie_1.mp4 -o ./results -n 100 -g 5 -s 20 -p 17,18,19

python inference_upscale_a_video.py \
-i ./inputs/old_movie_2.mp4 -o ./results -n 120 -g 6 -s 30 -p 8,10,12

python inference_upscale_a_video.py \
-i ./inputs/old_animation_1.mp4 -o ./results -n 120 -g 6 -s 20 --use_video_vae
```

If you notice any color discrepancies between the output and the input, you can set `--color_fix` to `"AdaIn"` or `"Wavelet"`. By default, it is set to `"None"`.



## ğŸï¸ YouHQ Dataset
The datasets are hosted on Google Drive

| Dataset | Link | Description|
| :----- | :--: | :---- | 
| YouHQ-Train | [Google Drive](https://drive.google.com/file/d/1f8g8gTHzQq-cKt4s94YQXDwJcdjL59lK/view?usp=sharing)| 38,576 videos for training, each of which has around 32 frames.|
| YouHQ40-Test| [Google Drive](https://drive.google.com/file/d/1rkeBQJMqnRTRDtyLyse4k6Vg2TilvTKC/view?usp=sharing) | 40 video clips for evaluation, each of which has around 32 frames.|

## ğŸ“‘ Citation

   If you find our repo useful for your research, please consider citing our paper:

   ```bibtex
   @inproceedings{zhou2024upscaleavideo,
      title={{Upscale-A-Video}: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution},
      author={Zhou, Shangchen and Yang, Peiqing and Wang, Jianyi and Luo, Yihang and Loy, Chen Change},
      booktitle={CVPR},
      year={2024}
   }
   ```


## ğŸ“ License

This project is licensed under <a rel="license" href="./LICENSE">NTU S-Lab License 1.0</a>. Redistribution and use should follow this license.


## ğŸ“§ Contact
If you have any questions, please feel free to reach us at `shangchenzhou@gmail.com` or `peiqingyang99@outlook.com`. 
